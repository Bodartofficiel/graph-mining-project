{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='https://netacad.centralesupelec.fr/img/cs.jpg' width=200></center>\n",
    "\n",
    "<h3><center>Clément VERON | Thomas Bodart | Marc Garcia</center></h3>\n",
    "<h1>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "<center>Graph mining project</center>\n",
    "<hr style=\" border:none; height:3px;\">\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline des ajouts :\n",
    "\n",
    "    - Analyse de la matrice de corrélation entre les différentes centralités\n",
    "\n",
    "    - Analyse de certains noeuds ayant de grande valeur de centralité\n",
    "\n",
    "    - Comparaison des rangs de centralités aux clusters calculés par k-means\n",
    "\n",
    "    - Calcul de la clique maximale et ajout de la comparaison avec l'algorithme des k-cliques\n",
    "\n",
    "    - Changement d'implémentation pour le calcul de l'embedding spectral avec choix judicieux du nombre de plus petits vecteurs propres sélectionnés.\n",
    "\n",
    "    - Comparaison des deux embeddings (spectral & Node2Vec) par deux approches : modularité du clustering et prédiction de lien.\n",
    "\n",
    "    - Justification des choix d'hyperparamètres pour les algorithmes de clustering (KMeans & DBSCAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline du projet :\n",
    "\n",
    "0- Installation des packages\n",
    "\n",
    "1- Chargement des données\n",
    "\n",
    "2- Centralités <br>\n",
    "2.1 - Deezer Europe Centralities<br>\n",
    "2.2 - Lastfm Graph Centralities\n",
    "\n",
    "3- Community detection<br>\n",
    "3.1- 8-clique community<br>\n",
    "3.2- Louvain communities\n",
    "\n",
    "4- Embeddings<br>\n",
    "4.1- Node2Vec embedding<br>\n",
    "4.2- Spectral embedding\n",
    "\n",
    "5- Comparaison des embeddings<br>\n",
    "5.1 Comparaison des clusters dans l'espace des embeddings<br>\n",
    "5.2 Link prediction\n",
    "\n",
    "6- GNN-based approach<br>\n",
    "6.1 Deezer<br>\n",
    "6.2 LastFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install networkx\n",
    "# ! pip install matplotlib\n",
    "# ! pip install scipy\n",
    "# ! pip install gensim\n",
    "# ! pip install node2vec\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install numpy \n",
    "# ! pip install tqdm\n",
    "# ! pip install seaborn\n",
    "# ! pip install torch\n",
    "# ! pip install random\n",
    "# ! pip install collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deezer graph :  Graph with 28281 nodes and 92752 edges\n",
      "Lastfm graph :  Graph with 7624 nodes and 27806 edges\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "file_dir = Path(os.getcwd()) \n",
    "lastfm_data_path = file_dir / \"lastfm_asia\"\n",
    "deezer_data_path = file_dir / \"deezer_europe\"\n",
    "\n",
    "assert os.path.isdir(\n",
    "    deezer_data_path\n",
    "), f\"Deezer data path is missing: {deezer_data_path}\"\n",
    "assert os.path.isdir(\n",
    "    lastfm_data_path\n",
    "), f\"LastFM data path is missing: {lastfm_data_path}\"\n",
    "\n",
    "lastfm_edges_path = lastfm_data_path / \"lastfm_asia_edges.csv\"\n",
    "deezer_edges_path = deezer_data_path / \"deezer_europe_edges.csv\"\n",
    "lastfm_nattr_path = lastfm_data_path / \"lastfm_asia_features.json\"\n",
    "deezer_nattr_path = deezer_data_path / \"deezer_europe_features.json\"\n",
    "\n",
    "assert os.path.exists(\n",
    "    deezer_edges_path\n",
    "), f\"Deezer edges file is missing: {deezer_edges_path}\"\n",
    "assert os.path.exists(\n",
    "    lastfm_edges_path\n",
    "), f\"LastFM edges file is missing: {lastfm_edges_path}\"\n",
    "\n",
    "\n",
    "def read_edges(file_path):\n",
    "    edges = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            source, target = line.strip().split(\",\")\n",
    "            edges.append((int(source), int(target)))\n",
    "    return edges\n",
    "\n",
    "\n",
    "lastfm_edges = read_edges(lastfm_edges_path)\n",
    "deezer_edges = read_edges(deezer_edges_path)\n",
    "\n",
    "deezer_graph: nx.graph.Graph = nx.from_edgelist(deezer_edges)\n",
    "lastfm_graph: nx.graph.Graph = nx.from_edgelist(lastfm_edges)\n",
    "\n",
    "with open(deezer_nattr_path, \"r\") as f:\n",
    "    deezer_node_attributes = json.load(f)\n",
    "\n",
    "with open(lastfm_nattr_path, \"r\") as f:\n",
    "    lastfm_node_attributes = json.load(f)\n",
    "\n",
    "for node_idx, node_features in deezer_node_attributes.items():\n",
    "    deezer_graph.nodes[int(node_idx)][\"feat\"] = node_features\n",
    "\n",
    "for node_idx, node_features in lastfm_node_attributes.items():\n",
    "    lastfm_graph.nodes[int(node_idx)][\"feat\"] = node_features\n",
    "\n",
    "\n",
    "print(\"Deezer graph : \", deezer_graph)\n",
    "print(\"Lastfm graph : \", lastfm_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Centralités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_centralities(graph):\n",
    "    centralities = {}\n",
    "\n",
    "    # Calcul de l'ensemble des centralités\n",
    "    degree_centrality = nx.degree_centrality(graph)\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(graph, max_iter=100, tol=1e-05)\n",
    "    katz_centrality = nx.katz_centrality(graph, alpha=0.02, beta=1.0, max_iter=1000, tol=1e-05)\n",
    "    pagerank_centrality = nx.pagerank(graph)\n",
    "    clustering_centrality = nx.clustering(graph)\n",
    "    neighborhood_connectivity = nx.average_neighbor_degree(graph)\n",
    "\n",
    "    # Création d'un dictionnaire des centralités\n",
    "    centrality_measures = {\n",
    "        \"degree\": degree_centrality,\n",
    "        \"eigenvector\": eigenvector_centrality,\n",
    "        \"katz\": katz_centrality,\n",
    "        \"pagerank\": pagerank_centrality,\n",
    "        \"clustering\": clustering_centrality,\n",
    "        \"neighborhood\": neighborhood_connectivity\n",
    "    }\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        centralities[node] = {}\n",
    "\n",
    "    # Calcul des rankings pour l'ensemble des centralités\n",
    "    for centrality_type, values in centrality_measures.items():\n",
    "        sorted_nodes = sorted(values, key=values.get, reverse=True) \n",
    "        ranking = {node: rank + 1 for rank, node in enumerate(sorted_nodes)} \n",
    "        \n",
    "        # Ajout des valeurs et du rang de chaque noeud pour toutes les centralités\n",
    "        for node in graph.nodes():\n",
    "            centralities[node][centrality_type] = {\n",
    "                \"rank\": ranking[node],\n",
    "                \"value\": values[node]\n",
    "            }\n",
    "    \n",
    "    return centralities\n",
    "\n",
    "deezer_centralities = compute_all_centralities(deezer_graph)\n",
    "lastfm_centralities = compute_all_centralities(lastfm_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Deezer Europe Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def extract_top_subgraph(graph, centrality_dict, centrality_type, top_n=50):\n",
    "    \"\"\"Extracts a subgraph with the top N nodes based on a chosen centrality measure.\"\"\"\n",
    "    centrality_values = {node: data[centrality_type][\"value\"] for node, data in centrality_dict.items()}\n",
    "    top_nodes = sorted(centrality_values, key=centrality_values.get, reverse=True)[:top_n]\n",
    "    return graph.subgraph(top_nodes), centrality_values\n",
    "\n",
    "def visualize_graph(graph, centrality_values, title=\"Graph Visualization\"):\n",
    "    \"\"\"Visualizes the graph with node color and size based on a chosen centrality measure.\"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Mapping des centralités avec des couleurs\n",
    "    centrality_array = np.array(list(centrality_values.values()))\n",
    "    norm = plt.Normalize(vmin=centrality_array.min(), vmax=centrality_array.max())\n",
    "    cmap = plt.cm.plasma \n",
    "    node_colors = [cmap(norm(centrality_values[node])) for node in graph.nodes()]\n",
    "\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "\n",
    "    nx.draw(graph, pos, with_labels=True, node_color=node_colors, edge_color=\"gray\",\n",
    "            font_size=10, cmap=cmap, ax=ax)\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([]) \n",
    "    plt.colorbar(sm, ax=ax, label=\"Centrality Value\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Select centrality type for visualization\n",
    "chosen_centralities = [\"degree\", \"eigenvector\", \"katz\", \"pagerank\", \"clustering\", \"neighborhood\"]\n",
    "\n",
    "for chosen_centrality in chosen_centralities:\n",
    "    subgraph, subgraph_centrality_values = extract_top_subgraph(deezer_graph, deezer_centralities, chosen_centrality)\n",
    "    visualize_graph(subgraph, subgraph_centrality_values, title=f\"Top 50 Nodes by {chosen_centrality.capitalize()} Centrality\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transformation en DataFrame\n",
    "rows = []\n",
    "for node, cent_values in deezer_centralities.items():\n",
    "    row = {'node': node}\n",
    "    for cent_name, metrics in cent_values.items():\n",
    "        row[f'{cent_name}_value'] = metrics['value']\n",
    "        row[f'{cent_name}_rank'] = metrics['rank']\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.set_index('node', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corrélation entre centralités (valeurs)\n",
    "value_cols = [col for col in df.columns if col.endswith('_value')]\n",
    "sns.heatmap(df[value_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Corrélation entre les valeurs des centralités\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice de corrélation des différentes centalités nous apporte différentes informations, on voit notamment que la degree_centrality, la katz_centrality et la pagerank_centrality sont extrêmement corrélées au contraire de la neighborhood_centrality et de la clustering_centrality ce qui provient du calcul intrinsèque des différentes centralités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "slope, intercept = np.polyfit(df['degree_value'], df['katz_value'], 1)\n",
    "\n",
    "print(f\"Coefficient directeur : {slope}\")\n",
    "print(f\"Ordonnée à l'origine : {intercept}\")\n",
    "\n",
    "sns.scatterplot(x='degree_value', y='katz_value', data=df)\n",
    "x_vals = np.linspace(df['degree_value'].min(), df['degree_value'].max(), 100)\n",
    "y_vals = slope * x_vals + intercept\n",
    "plt.plot(x_vals, y_vals, color='red', label='Régression linéaire')\n",
    "plt.title(\"Degré vs Katz\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici bien la corrélation entre les deux centralités avec un facteur 4.65 ce qui signifie que les deux centralités amènent la même information de manière générale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = df[value_cols]\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Nœuds avec des valeurs particulièrement fortes/faibles\n",
    "top_degree = df['degree_rank'].nsmallest(10)\n",
    "low_pagerank = df['pagerank_rank'].nlargest(10)\n",
    "\n",
    "special_nodes = list(set(top_degree.index) | set(low_pagerank.index))\n",
    "df.loc[special_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le noeud 867 possède la plus grande degree_centrality et katz_centrality, il est également classé deuxième sur la pagerank_centrality. Néanmoins, il possède un rang assez peu élevé pour les deux dernières centralité.\n",
    "\n",
    "On voit également que les différents noeuds soit bien classés soit mal classés par rapport aux premières centralités sont dans des clusters différents resp cluster 2 et cluster 0 ce qui corrobore nos mesures de centralités."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Lastfm Graph Centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select centrality type for visualization\n",
    "chosen_centralities = [\"degree\", \"eigenvector\", \"katz\", \"pagerank\", \"clustering\", \"neighborhood\"]\n",
    "\n",
    "for chosen_centrality in chosen_centralities:\n",
    "    subgraph, subgraph_centrality_values = extract_top_subgraph(lastfm_graph, lastfm_centralities, chosen_centrality)\n",
    "    visualize_graph(subgraph, subgraph_centrality_values, title=f\"Top 50 Nodes by {chosen_centrality.capitalize()} Centrality\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Transformation en DataFrame\n",
    "rows = []\n",
    "for node, cent_values in lastfm_centralities.items():\n",
    "    row = {'node': node}\n",
    "    for cent_name, metrics in cent_values.items():\n",
    "        row[f'{cent_name}_value'] = metrics['value']\n",
    "        row[f'{cent_name}_rank'] = metrics['rank']\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.set_index('node', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Corrélation entre centralités (valeurs)\n",
    "value_cols = [col for col in df.columns if col.endswith('_value')]\n",
    "sns.heatmap(df[value_cols].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Corrélation entre les valeurs des centralités\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve des résultats similaires au premier graph ce qui est cohérent puisque les deux graphs sont très similaires. Encore une fois on peut regrouper les quatres premières centralités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "slope, intercept = np.polyfit(df['degree_value'], df['katz_value'], 1)\n",
    "\n",
    "print(f\"Coefficient directeur : {slope}\")\n",
    "print(f\"Ordonnée à l'origine : {intercept}\")\n",
    "\n",
    "sns.scatterplot(x='degree_value', y='katz_value', data=df)\n",
    "x_vals = np.linspace(df['degree_value'].min(), df['degree_value'].max(), 100)\n",
    "y_vals = slope * x_vals + intercept\n",
    "plt.plot(x_vals, y_vals, color='red', label='Régression linéaire')\n",
    "plt.title(\"Degré vs Katz\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve une forte corrélation entre la degree_centrality et la katz_centrality, le rapport entre les deux centralités est proche de celui précedemment trouvé (proche de 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = df[value_cols]\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Nœuds avec des valeurs particulièrement fortes/faibles\n",
    "top_degree = df['degree_rank'].nsmallest(10)\n",
    "low_pagerank = df['pagerank_rank'].nlargest(10)\n",
    "\n",
    "special_nodes = list(set(top_degree.index) | set(low_pagerank.index))\n",
    "df.loc[special_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le noeud 7237 a le premier rang pour ce qui est du degree_rank, eigenvector_rank et katz_rank (4ème sur pagerank). On le retrouve sur les plots précédents et on voit bien sur ceux-ci qu'il est extrêmement central. Ainsi il s'agit d'un utilisateur suivi par de nombreux autres utilisateurs, une sorte d'influenceur du réseau social.\n",
    "\n",
    "Les neouds ayant une centralité grande par rapport aux autres ont été clusterisés par l'algorithme du k-means (k=3) dans le cluster 0 à l'inverse ceux avec une centralité faible ont été affecté au cluster 1.\n",
    "\n",
    "Le cluster 2 quant à lui n'apparait pas ce qui signifie que les noeuds centraux sont clusterisés dans le cluster 0, les noeuds périphériques (ie non centraux) dans le cluster 1 et les autres dans le cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1- 8-clique community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Calcul des communautés 8-cliques\n",
    "communities = sorted(list(nx.community.k_clique_communities(deezer_graph, 8)), key=len, reverse=True)\n",
    "print(\"8-clique communautés calculées\")\n",
    "print(f\"Nombre de 8-clique communautés : {len(communities)}\")\n",
    "\n",
    "# Sélectionner les k plus grandes communautés à afficher\n",
    "k = min(2, len(communities))\n",
    "communities = communities[:k]\n",
    "\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y']) \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (community, color) in enumerate(zip(communities, colors), start=1):\n",
    "    \n",
    "    subgraph = deezer_graph.subgraph(community)\n",
    "    pos = nx.spring_layout(subgraph, seed=42)\n",
    "    \n",
    "    # Dessiner le sous-graphe\n",
    "    plt.subplot(1, k, i)\n",
    "    nx.draw(subgraph, pos, node_color=color, edge_color='gray', with_labels=False, node_size=80)\n",
    "    plt.title(f\"Communauté {i} ({len(community)} nœuds)\")\n",
    "\n",
    "plt.suptitle(\"Visualisation des plus grandes communautés 8-cliques\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "maximal_cliques = list(nx.find_cliques(deezer_graph))\n",
    "largest_clique = max(maximal_cliques, key=len)\n",
    "\n",
    "print(f\"Plus grande clique trouvée (taille {len(largest_clique)}):\", largest_clique)\n",
    "\n",
    "# Sous-graphe induit par la clique\n",
    "clique_subgraph = deezer_graph.subgraph(largest_clique)\n",
    "\n",
    "# Plot\n",
    "pos = nx.spring_layout(clique_subgraph, seed=42)\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(clique_subgraph, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=700, font_size=10)\n",
    "plt.title(f\"Plus grande clique (taille {len(largest_clique)})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2- Louvain communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = sorted(list(nx.community.louvain_communities(deezer_graph)), key=len, reverse=True)\n",
    "print(\"Nombre de communautés de Louvain trouvées : \", len(communities))\n",
    "\n",
    "# Sélectionner les k plus grandes communautés à afficher\n",
    "k = min(1, len(communities))\n",
    "communities = communities[:k]\n",
    "\n",
    "colors = itertools.cycle(['r', 'g', 'b', 'c', 'm', 'y']) \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (community, color) in enumerate(zip(communities, colors), start=1):\n",
    "    subgraph = deezer_graph.subgraph(community)\n",
    "    pos = nx.spring_layout(subgraph)\n",
    "    \n",
    "    # Dessiner le sous-graphe\n",
    "    nx.draw(subgraph, pos, node_color=color, edge_color='gray', with_labels=False, node_size=50)\n",
    "    plt.title(f\"Communauté {i} ({len(community)} nœuds)\")\n",
    "\n",
    "plt.suptitle(\"Visualisation de la plus grande communauté de Louvain\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La communauté de Louvain trouvée est très grande 4607 noeuds sur les 28000 que possède le graphe (16% des noeuds) cela signifie que tous ces noeuds sont fortement connectés entre eux. Il s'agit là d'un cas classique pour des réseaux réels, les utilisateurs ont un comportement homogène cela signifie que ce sont sûrement les utilisateurs les plus actifs du réseau social et que ceux ci suivent et sont suivis par de nombreux utilisateurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "used_graph = deezer_graph\n",
    "\n",
    "percent_nodes = 1 # Decrease for faster execution\n",
    "\n",
    "if percent_nodes != 1 :\n",
    "    n_nodes = int(used_graph.number_of_nodes() * percent_nodes)\n",
    "    print(f\"Selected nodes: {n_nodes} ({percent_nodes*100:.0f}%)\")\n",
    "\n",
    "    # Random walk sampling\n",
    "    def random_walk_sampling(graph, start_node, walk_len, damping_factor=0.15):\n",
    "        visited = {start_node}\n",
    "        current_node = start_node\n",
    "        for _ in range(walk_len):\n",
    "            neighbors = list(graph.neighbors(current_node))\n",
    "            if np.random.random()>damping_factor and neighbors:\n",
    "                current_node = np.random.choice(neighbors)\n",
    "                visited.add(current_node)\n",
    "            else :\n",
    "                current_node = np.random.choice(graph.nodes)\n",
    "        return(visited)\n",
    "\n",
    "    random_nodes = random_walk_sampling(used_graph,start_node=np.random.choice(used_graph.nodes),walk_len=n_nodes)\n",
    "    used_graph = used_graph.subgraph(random_nodes)\n",
    "\n",
    "print(used_graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1- Node2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node2vec embedding\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "class Node2vec:\n",
    "    def __init__(self, graph: nx.Graph, dimensions: int = 128, walk_length: int = 10,\n",
    "                num_walks: int = 20, p: float = 1, q: float = 1, window_size: int = 5,\n",
    "                min_count: int = 1, workers: int = 1, epochs: int = 10, quiet: bool = False):\n",
    "        \n",
    "        self.graph = graph\n",
    "        self.dimensions =  dimensions\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.quiet = quiet\n",
    "\n",
    "        self.p_graph = defaultdict(lambda: {'probabilities': {}})\n",
    "\n",
    "        self._preprocess_modified_probs()\n",
    "        if not self.quiet:\n",
    "            print(\"Preprocessing des probabilités modifiées terminés.\")\n",
    "        self.walks = self._random_walk()\n",
    "        if not self.quiet:\n",
    "            print(\"Random walks calculées.\")\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "\n",
    "        print(\"Entrainment du modèle Word2Vec pour calculer l'embedding.\")\n",
    "        self._train()\n",
    "        print(\"Modèle entrainé.\")\n",
    "\n",
    "\n",
    "    def _preprocess_modified_probs(self):\n",
    "        for origin_node in self.graph.nodes():\n",
    "\n",
    "            for current_node in self.graph.neighbors(origin_node):\n",
    "                unnormalized_probs = []\n",
    "\n",
    "                for next_node in self.graph.neighbors(current_node):\n",
    "                    if next_node == origin_node:\n",
    "                        modified_prob = 1 / self.p\n",
    "                    elif next_node in self.graph.neighbors(origin_node):\n",
    "                        modified_prob = 1\n",
    "                    else:\n",
    "                        modified_prob = 1 / self.q\n",
    "\n",
    "                    unnormalized_probs.append(modified_prob)\n",
    "\n",
    "                # Normalisation\n",
    "                if unnormalized_probs:\n",
    "                    Z = abs(sum(unnormalized_probs))\n",
    "                    normalized_probs = np.array([prob / Z for prob in unnormalized_probs])\n",
    "                    self.p_graph[current_node]['probabilities'][origin_node] = normalized_probs\n",
    "\n",
    "    def _random_walk(self) -> list:\n",
    "\n",
    "        def _alias_setup(probs):\n",
    "\n",
    "            N = len(probs)\n",
    "            scaled_probs = [p * N for p in probs]\n",
    "            prob = [0]*N\n",
    "            alias = np.zeros(N, dtype=int) \n",
    "\n",
    "            small = []\n",
    "            large = []\n",
    "\n",
    "            for i, sp in enumerate(scaled_probs):\n",
    "                if sp < 1: \n",
    "                    small.append(i)\n",
    "                else:\n",
    "                    large.append(i)\n",
    "\n",
    "            while small and large:\n",
    "                small_idx = small.pop()\n",
    "                large_idx = large.pop()\n",
    "\n",
    "                prob[small_idx] = scaled_probs[small_idx]\n",
    "                alias[small_idx] = large_idx\n",
    "                scaled_probs[large_idx] += (scaled_probs[small_idx] - 1)\n",
    "\n",
    "                if scaled_probs[large_idx] < 1:\n",
    "                    small.append(large_idx)\n",
    "                else:\n",
    "                    large.append(large_idx)\n",
    "\n",
    "            return prob, alias\n",
    "\n",
    "        def _choose_next(prob, alias):\n",
    "            N = len(prob)\n",
    "            i = np.random.randint(N)\n",
    "            return i if np.random.rand() < prob[i] else alias[i]\n",
    "\n",
    "        def _next_neighbor(current_node, origin_node, p_graph):\n",
    "            neighbors = list(self.graph.neighbors(current_node))\n",
    "            if not neighbors:\n",
    "                return current_node\n",
    "            \n",
    "            prob = p_graph[current_node]['probabilities'][origin_node]\n",
    "            prob, alias = _alias_setup(prob)\n",
    "\n",
    "            next = _choose_next(prob, alias)\n",
    "            return neighbors[next]\n",
    "        \n",
    "        def _compute_first_travel(node, p_graph):\n",
    "            neighbors = list(self.graph.neighbors(node))\n",
    "            return random.choice(neighbors) if neighbors else node\n",
    "\n",
    "        walks = []\n",
    "        for iter in range(self.num_walks):\n",
    "\n",
    "            shuffled_nodes = list(self.graph.nodes())\n",
    "            random.shuffle(shuffled_nodes)\n",
    "\n",
    "            for node in shuffled_nodes:\n",
    "                walk = [node]\n",
    "                walk.append(_compute_first_travel(node, self.p_graph))\n",
    "                for j in range(self.walk_length-1):\n",
    "                    current_node, origin_node = walk[-1], walk[-2]\n",
    "                    next = _next_neighbor(current_node, origin_node, self.p_graph)\n",
    "                    walk.append(next)\n",
    "                walks.append(walk)\n",
    "        return walks\n",
    "    \n",
    "    def _train(self):\n",
    "        walks_str = [[str(node) for node in walk] for walk in self.walks]\n",
    "        self.model = Word2Vec(\n",
    "            sentences=walks_str,\n",
    "            vector_size=self.dimensions,\n",
    "            window=self.window_size,\n",
    "            min_count=self.min_count,\n",
    "            sg=1,\n",
    "            workers=self.workers,\n",
    "            epochs=self.epochs\n",
    "        )\n",
    "\n",
    "    def get_embedding(self, node):\n",
    "        return self.model.wv[str(node)] if str(node) in self.model.wv else None\n",
    "    \n",
    "    def cosine_similarity(self, node1, node2):\n",
    "        emb1 = self.get_embedding(node1)\n",
    "        emb2 = self.get_embedding(node2)\n",
    "        \n",
    "        if emb1 is None or emb2 is None:\n",
    "            return None\n",
    "\n",
    "        return 1 - cosine(emb1, emb2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import node2vec as n2v\n",
    "\n",
    "# Embedding Node2vec ci-dessus\n",
    "node2vec_embedding = Node2vec(used_graph, p=0.2, q=1.2, workers=4, epochs=20)\n",
    "print(\"Premier embedding calculé.\")\n",
    "\n",
    "# Embedding Node2vec de https://github.com/eliorc\n",
    "node2vec_lib = n2v.node2vec.Node2Vec(used_graph, p=0.2, q=1.2, workers=4)\n",
    "node2vec_model_lib =  node2vec_lib.fit(window=5, min_count=1)\n",
    "print(\"Second embedding calculé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2- Spectral embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer la matrice laplacienne\n",
    "L = nx.laplacian_matrix(used_graph).astype(float)\n",
    "\n",
    "# Calculer les valeurs propres et vecteurs propres\n",
    "k = 100\n",
    "eigenvalues, eigenvectors = linalg.eigsh(L, k=k, which='SM')\n",
    "tolerance = 1e-14\n",
    "eigenvalues = np.real(eigenvalues)\n",
    "eigenvalues[np.abs(eigenvalues) < tolerance] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs propres pour trouver un gap spectral (vp_k << vp_k+1)\n",
    "plt.scatter(np.arange(k), eigenvalues)\n",
    "plt.axvline(x=71, color='red', linestyle='--', label='vp 71 (index)')\n",
    "plt.axhline(y=eigenvalues[71], color='blue', linestyle='--', label=f'vp 71 (valeur: {eigenvalues[71]:.4f})')\n",
    "plt.title(\"Valeurs propres\")\n",
    "plt.xlabel(\"Numéro valeur propre\")\n",
    "plt.ylabel(\"Valeur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un grand écart entre les valeurs propres numéro 71 et 72. On choisit donc de ne sélectionner que les 71 plus petites vp pour réaliser l'embedding (moins celle qui vaut 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les valeurs propres et vecteurs propres\n",
    "chosen_k = 72\n",
    "eigenvalues, eigenvectors = linalg.eigsh(L, k=chosen_k, which='SM')\n",
    "# eigenvalues, eigenvectors = eigenvalues[chosen_k], eigenvectors[:, :chosen_k]\n",
    "\n",
    "# Sélectionner les vecteurs propres correspondant aux plus petites valeurs propres (sauf la première qui est 0)\n",
    "embedding_spectral = eigenvectors[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_node2vec = np.array([node2vec_embedding.get_embedding(node) for node in range(n_nodes)])\n",
    "print(embedding_node2vec.shape)\n",
    "print(embedding_spectral.shape)\n",
    "# (28281, 128)\n",
    "# (28281, 71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Comparaison des embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1- Comparaison des clusters dans l'espace des embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Les valeurs suivantes sont obtenues pour le graph deezer_graph entier et pourrait être différentes si l'on considère un sous-graphe.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détermination des hyperparamètres des algorithmes de clustering : number of clusters (k, KMeans), maximum distance (eps, DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "n_nodes = embedding_node2vec.shape[0]\n",
    "# Fonction pour sélectionner le nombre de clusters pour K-means\n",
    "def select_kmeans_n_clusters(embedding, explored_range=np.arange(n_nodes//100, n_nodes//10, 500)):\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    for n_clusters in tqdm(explored_range):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        labels = kmeans.fit_predict(embedding)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(embedding, labels))\n",
    "\n",
    "    # Méthode du coude\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(explored_range, inertia, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method')\n",
    "\n",
    "    # Silhouette Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(explored_range, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.show()\n",
    "\n",
    "    # Retourner le nombre de clusters optimal basé sur le silhouette score\n",
    "    optimal_n_clusters = np.argmax(silhouette_scores) + 2\n",
    "    return optimal_n_clusters\n",
    "\n",
    "\n",
    "# Sélection des hyperparamètres pour K-means\n",
    "# Successivement cherché le coude et pris en compte le max de silhouette avec des intervalles de plus en plus précis\n",
    "# explored_range = np.arange(50, 175, 30)\n",
    "# print(explored_range, len(explored_range))\n",
    "# select_kmeans_n_clusters(embedding_spectral, explored_range=explored_range)\n",
    "optimal_n_clusters_spectral = 100 # prise en compte des deux indicateurs \n",
    "\n",
    "# explored_range = np.arange(50, 200, 30)\n",
    "# print(explored_range, len(explored_range))\n",
    "# select_kmeans_n_clusters(embedding_node2vec, explored_range=explored_range)\n",
    "optimal_n_clusters_node2vec = optimal_n_clusters_spectral\n",
    "# Aucun coude d'inertie ou pic d'inertie. Réutilisation du même paramètre pour simplifier la comparaison.\n",
    "\n",
    "\n",
    "print(\"Optimal number of clusters for Spectral Embedding (K-means):\", optimal_n_clusters_spectral)\n",
    "print(\"Optimal number of clusters for Node2Vec Embedding (K-means):\", optimal_n_clusters_node2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour sélectionner eps pour DBSCAN\n",
    "def select_dbscan_eps(embedding, min_samples=5):\n",
    "    neigh = NearestNeighbors(n_neighbors=min_samples)\n",
    "    nbrs = neigh.fit(embedding)\n",
    "    distances, _ = nbrs.kneighbors(embedding)\n",
    "\n",
    "    # k-distance plot\n",
    "    distances = np.sort(distances[:, min_samples - 1], axis=0)\n",
    "    plt.plot(distances)\n",
    "    plt.axhline(y=distances[int(len(distances) * 0.9)], color='red', linestyle='--', label=f'point à 90%')\n",
    "    plt.xlabel('Points sorted by distance')\n",
    "    plt.ylabel(f'{min_samples}-distance')\n",
    "    plt.title('k-distance plot')\n",
    "    plt.show()\n",
    "\n",
    "    # Retourner eps basé sur le coude du k-distance plot\n",
    "    # Cela peut être fait manuellement ou en utilisant une heuristique\n",
    "    eps = distances[int(len(distances) * 0.9)]  # Exemple heuristique\n",
    "    return eps\n",
    "\n",
    "# Sélection des hyperparamètres pour DBSCAN\n",
    "optimal_eps_spectral = select_dbscan_eps(embedding_spectral)\n",
    "optimal_eps_node2vec = select_dbscan_eps(embedding_node2vec)\n",
    "\n",
    "\n",
    "print(\"Optimal eps for Spectral Embedding (DBSCAN):\", optimal_eps_spectral)\n",
    "print(\"Optimal eps for Node2Vec Embedding (DBSCAN):\", optimal_eps_node2vec)\n",
    "\n",
    "# Optimal eps for Spectral Embedding (DBSCAN): 0.0017430409564679395\n",
    "# Optimal eps for Node2Vec Embedding (DBSCAN): 4.739624500274658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from networkx.algorithms.community import modularity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_clustering(embedding, graph, method='kmeans', n_clusters=2, eps=0.5, min_samples=5):\n",
    "    if method == 'kmeans':\n",
    "        clustering = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        labels = clustering.fit_predict(embedding)\n",
    "    elif method == 'dbscan':\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = clustering.fit_predict(embedding)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'kmeans' or 'dbscan'\")\n",
    "\n",
    "    # Calculer la modularité\n",
    "    modularity_score = modularity(graph, [set(np.where(labels == i)[0]) for i in np.unique(labels) if i != -1]+[{j} for j,i in enumerate(labels) if i == -1])\n",
    "    return labels, modularity_score\n",
    "\n",
    "# Appliquer K-means et DBSCAN sur les embeddings spectral et node2vec\n",
    "labels_spectral_kmeans, modularity_spectral_kmeans = evaluate_clustering(embedding_spectral, used_graph, method='kmeans', n_clusters=optimal_n_clusters_spectral, eps=optimal_eps_spectral)\n",
    "labels_node2vec_kmeans, modularity_node2vec_kmeans = evaluate_clustering(embedding_node2vec, used_graph, method='kmeans', n_clusters=optimal_n_clusters_node2vec, eps=optimal_eps_node2vec)\n",
    "\n",
    "labels_spectral_dbscan, modularity_spectral_dbscan = evaluate_clustering(embedding_spectral, used_graph, method='dbscan', n_clusters=optimal_n_clusters_spectral, eps=optimal_eps_spectral)\n",
    "labels_node2vec_dbscan, modularity_node2vec_dbscan = evaluate_clustering(embedding_node2vec, used_graph, method='dbscan', n_clusters=optimal_n_clusters_node2vec, eps=optimal_eps_node2vec)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"K-means - Spectral Embedding: Modularity =\", modularity_spectral_kmeans)\n",
    "print(\"K-means - Node2Vec Embedding: Modularity =\", modularity_node2vec_kmeans)\n",
    "print(\"DBSCAN - Spectral Embedding: Modularity =\", modularity_spectral_dbscan)\n",
    "print(\"DBSCAN - Node2Vec Embedding: Modularity =\", modularity_node2vec_dbscan)\n",
    "\n",
    "# K-means - Spectral Embedding: Modularity = 0.00010131904420685507\n",
    "# K-means - Node2Vec Embedding: Modularity = 0.5823188650906954\n",
    "# DBSCAN - Spectral Embedding: Modularity = -0.0006096588445744115\n",
    "# DBSCAN - Node2Vec Embedding: Modularity = 0.000929680962462588\n",
    "\n",
    "# Visualisation des clusters pour K-means\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(embedding_spectral[:, 0], embedding_spectral[:, 1], c=labels_spectral_kmeans, cmap='viridis')\n",
    "plt.title(\"K-means - Spectral Embedding\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(embedding_node2vec[:, 0], embedding_node2vec[:, 1], c=labels_node2vec_kmeans, cmap='viridis')\n",
    "plt.title(\"K-means - Node2Vec Embedding\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualisation des clusters pour DBSCAN\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(embedding_spectral[:, 0], embedding_spectral[:, 1], c=labels_spectral_dbscan, cmap='viridis')\n",
    "plt.title(\"DBSCAN - Spectral Embedding\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(embedding_node2vec[:, 0], embedding_node2vec[:, 1], c=labels_node2vec_dbscan, cmap='viridis')\n",
    "plt.title(\"DBSCAN - Node2Vec Embedding\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les différentes modularités sont extrêmement faibles, sauf KMeans-Node2Vec. Cela signifie que les clusters ne sont pas une bonne partition des graphs. Cela pourrait être dû aux embeddings ou alors au choix des hyperparamètres de clustering. <br>\n",
    "Node2Vec semble tout de même supérieur par cette étude car cet embedding obtient la seule valeur assez grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2- Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "\n",
    "# Fonction pour créer des paires de nœuds et leurs étiquettes (1 si lien, 0 sinon)\n",
    "def create_edge_samples(graph, embedding):\n",
    "    edges = list(graph.edges())\n",
    "    non_edges = list(nx.non_edges(graph))\n",
    "\n",
    "    # Échantillonner un nombre égal de liens et de non-liens\n",
    "    non_edges_sample = random.sample(non_edges, len(edges))\n",
    "\n",
    "    # Créer les features pour les paires de nœuds\n",
    "    edge_features = [np.abs(embedding[u] - embedding[v]) for u, v in edges]\n",
    "    non_edge_features = [np.abs(embedding[u] - embedding[v]) for u, v in non_edges_sample]\n",
    "\n",
    "    # Créer les labels\n",
    "    edge_labels = [1] * len(edges)\n",
    "    non_edge_labels = [0] * len(non_edges_sample)\n",
    "\n",
    "    # Combiner les features et les labels\n",
    "    features = np.array(edge_features + non_edge_features)\n",
    "    labels = np.array(edge_labels + non_edge_labels)\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# Créer les échantillons pour les embeddings spectral et node2vec\n",
    "features_spectral, labels_spectral = create_edge_samples(used_graph, embedding_spectral)\n",
    "features_node2vec, labels_node2vec = create_edge_samples(used_graph, embedding_node2vec)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train_spectral, X_test_spectral, y_train_spectral, y_test_spectral = train_test_split(features_spectral, labels_spectral, test_size=0.2, random_state=42)\n",
    "X_train_node2vec, X_test_node2vec, y_train_node2vec, y_test_node2vec = train_test_split(features_node2vec, labels_node2vec, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner un modèle de régression logistique pour la prédiction de liens\n",
    "model_spectral = LogisticRegression()\n",
    "model_spectral.fit(X_train_spectral, y_train_spectral)\n",
    "y_pred_spectral = model_spectral.predict_proba(X_test_spectral)[:, 1]\n",
    "\n",
    "model_node2vec = LogisticRegression()\n",
    "model_node2vec.fit(X_train_node2vec, y_train_node2vec)\n",
    "y_pred_node2vec = model_node2vec.predict_proba(X_test_node2vec)[:, 1]\n",
    "\n",
    "# Évaluer les performances avec l'AUC-ROC\n",
    "auc_spectral = roc_auc_score(y_test_spectral, y_pred_spectral)\n",
    "auc_node2vec = roc_auc_score(y_test_node2vec, y_pred_node2vec)\n",
    "\n",
    "print(\"AUC-ROC pour Spectral Embedding:\", auc_spectral)\n",
    "print(\"AUC-ROC pour Node2Vec Embedding:\", auc_node2vec)\n",
    "\n",
    "# AUC-ROC pour Spectral Embedding: 0.5073123081402587\n",
    "# AUC-ROC pour Node2Vec Embedding: 0.9926934878490318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le modèle ne réussit pas à apprendre avec l'embedding spectral (50% d'AUC-ROC cad aléatoire). Tandis que le modèle est presque parfait pour l'embedding Node2Vec (99%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusion de cette partie, au vue des score de modularité et de prédiction de liens, l'embedding Node2Vec ressort bien plus performant que celui spectral. Il faudrait envisager d'autres mesures pour être plus confiant dans cette affirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- GNN-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[28281, 1715], edge_index=[2, 185504], y=[28281])\n",
      "Data(x=[7624, 944], edge_index=[2, 55612], y=[7624])\n"
     ]
    }
   ],
   "source": [
    "from graph_analysis.get_graphs import (\n",
    "    deezer_pyg,\n",
    "    lastfm_pyg,\n",
    "    max_deezer_features,\n",
    "    max_lastfm_features,\n",
    "    max_lastfm_target,\n",
    ")\n",
    "import torch_geometric.nn as gnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Node : person. Edge : follow relationship. Features : likes.\n",
    "print(deezer_pyg)  # Target : gender\n",
    "print(lastfm_pyg)  # Target : country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_songs, emb_dim, hidden_dim, out_channels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_songs + 1, emb_dim, padding_idx=num_songs)\n",
    "        self.conv1 = gnn.GCNConv(emb_dim, hidden_dim)\n",
    "        self.conv2 = gnn.GCNConv(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        liked_lists = graph.x\n",
    "        batch_embeddings = []\n",
    "\n",
    "        for liked in liked_lists:\n",
    "            song_embs = self.embedding(liked)\n",
    "            mean_emb = song_embs.mean(dim=0)\n",
    "            batch_embeddings.append(mean_emb)\n",
    "\n",
    "        x = torch.stack(batch_embeddings)  # shape [num_nodes, emb_dim]\n",
    "\n",
    "        x = self.conv1(x, graph.edge_index).relu()\n",
    "        x = self.conv2(x, graph.edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "deezer_model_1 = GCN(\n",
    "    num_songs=max_deezer_features,\n",
    "    emb_dim=30,\n",
    "    hidden_dim=100,\n",
    "    out_channels=1,\n",
    ").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les GCN donnent une représentation compact des goûts musicaux des gens en agrégant les embeddings de chacun des utilisateurs partagant au moins un artiste liké en commun. Ainsi grâce aux voisins (ici des utilisateurs ayant liké les mêmes artistes) on peut en apprendre plus sur les goûts musicaux d'un utilisateur car cela modélise des relations sociales. De plus rien ne sert d'ajouter trop de couche à ce modèle car les GCN propagent bien l'information (message passing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, num_songs, emb_dim, hidden_dim, out_channels):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_songs + 1, emb_dim, padding_idx=num_songs)\n",
    "        self.conv1 = gnn.SAGEConv(emb_dim, hidden_dim, normalize=True)\n",
    "        self.conv2 = gnn.SAGEConv(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        liked_lists = graph.x\n",
    "        batch_embeddings = []\n",
    "\n",
    "        for liked in liked_lists:\n",
    "            song_embs = self.embedding(liked)\n",
    "            mean_emb = song_embs.mean(dim=0)\n",
    "            batch_embeddings.append(mean_emb)\n",
    "\n",
    "        x = torch.stack(batch_embeddings)  # shape [num_nodes, emb_dim]\n",
    "\n",
    "        x = self.conv1(x, graph.edge_index).relu()\n",
    "        x = self.conv2(x, graph.edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "deezer_model_2 = SAGE(\n",
    "    num_songs=max_deezer_features,\n",
    "    emb_dim=40,\n",
    "    hidden_dim=100,\n",
    "    out_channels=1,\n",
    ").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le modèle SAGE, l'agrégation est différente (concaténation avec le noeud courant). La couche du réseau de neurone SAGEConv permet de mieux capturer des relations asymétriques dans notre exemple cela pourrait être des utilisateurs influencant les autres par leur propre goûts musicaux voire carrément des artistes présent sur le réseau social.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1- Deezer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_ratio = 0.8\n",
    "num_nodes = deezer_pyg.x.shape[0]\n",
    "num_train = int(num_nodes * train_ratio)\n",
    "idx = [i for i in range(num_nodes)]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "indices = list(range(num_nodes))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * num_nodes)\n",
    "train_mask[indices[:split]] = True\n",
    "test_mask[indices[split:]] = True\n",
    "\n",
    "deezer_pyg.train_mask = train_mask\n",
    "deezer_pyg.test_mask = test_mask\n",
    "print(train_mask)\n",
    "print(test_mask)\n",
    "# Get class balance\n",
    "class_0_count = (deezer_pyg.y == 0).sum().item()\n",
    "class_1_count = (deezer_pyg.y == 1).sum().item()\n",
    "total = class_0_count + class_1_count\n",
    "print(\n",
    "    f\"Class balance: {class_0_count/total:.2%} class 0, {class_1_count/total:.2%} class 1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().to(\"mps\")\n",
    "optimizer = torch.optim.Adam(deezer_model_1.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "deezer_pyg = deezer_pyg.to(\"mps\")\n",
    "\n",
    "for epoch in tqdm(range(20), desc=\"Training\"):\n",
    "    deezer_model_1.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = deezer_model_1(deezer_pyg).squeeze(-1)\n",
    "    loss = criterion(\n",
    "        out[deezer_pyg.train_mask], deezer_pyg.y[deezer_pyg.train_mask].float()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.sigmoid() > 0.5\n",
    "        acc = (\n",
    "            (pred[deezer_pyg.test_mask] == deezer_pyg.y[deezer_pyg.test_mask].float())\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.sigmoid() > 0.5\n",
    "        acc = (\n",
    "            (pred[deezer_pyg.test_mask] == deezer_pyg.y[deezer_pyg.test_mask].float())\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss().to(\"mps\")\n",
    "optimizer = torch.optim.Adam(deezer_model_2.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "deezer_pyg = deezer_pyg.to(\"mps\")\n",
    "\n",
    "for epoch in tqdm(range(20), desc=\"Training\"):\n",
    "    deezer_model_2.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = deezer_model_2(deezer_pyg).squeeze(-1)\n",
    "    loss = criterion(\n",
    "        out[deezer_pyg.train_mask], deezer_pyg.y[deezer_pyg.train_mask].float()\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.sigmoid() > 0.5\n",
    "        acc = (\n",
    "            (pred[deezer_pyg.test_mask] == deezer_pyg.y[deezer_pyg.test_mask].float())\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.sigmoid() > 0.5\n",
    "        acc = (\n",
    "            (pred[deezer_pyg.test_mask] == deezer_pyg.y[deezer_pyg.test_mask].float())\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La loss baisse très lentement, ce qui indique que le modèle apprend un peu, et probablement de manière très limitée. De plus l’accuracy plafonne à ~55.7%, ce qui est à peine mieux que le hasard (le jeu de données étant légèrement déséquilibré : 55.67 % vs 44.33 %)\n",
    "\n",
    "Les utilisateurs connectés dans le graphe n’ont pas un genre cohérent entre eux et l'information apporté par la moyenne des embeddings des sons qu'ils aiment ne semblent pas suffisant pour déterminer leur genre. \n",
    "\n",
    "On pouvait s'attendre à ce genre de résultats mais on peut au moins en tirer que les goûts musicaux des gens n'est pas un bon indicateur de leur genre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2- LastFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  ...,  True, False,  True])\n",
      "tensor([False, False, False,  ..., False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Split the data\n",
    "train_ratio = 0.8\n",
    "num_nodes = lastfm_pyg.x.shape[0]\n",
    "num_train = int(num_nodes * train_ratio)\n",
    "idx = [i for i in range(num_nodes)]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "indices = list(range(num_nodes))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * num_nodes)\n",
    "train_mask[indices[:split]] = True\n",
    "test_mask[indices[split:]] = True\n",
    "\n",
    "lastfm_pyg.train_mask = train_mask\n",
    "lastfm_pyg.test_mask = test_mask\n",
    "print(train_mask)\n",
    "print(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 1/20 [00:01<00:25,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 2.8903, Test Acc: 0.0295\n",
      "Epoch 000, Loss: 2.8903, Test Acc: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 5/20 [00:06<00:18,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004, Loss: 2.8850, Test Acc: 0.1161\n",
      "Epoch 004, Loss: 2.8850, Test Acc: 0.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 9/20 [00:11<00:13,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008, Loss: 2.8797, Test Acc: 0.0977\n",
      "Epoch 008, Loss: 2.8797, Test Acc: 0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 13/20 [00:16<00:09,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012, Loss: 2.8742, Test Acc: 0.1043\n",
      "Epoch 012, Loss: 2.8742, Test Acc: 0.1043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 17/20 [00:22<00:04,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016, Loss: 2.8683, Test Acc: 0.1180\n",
      "Epoch 016, Loss: 2.8683, Test Acc: 0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:27<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_lastfm_1 = GCN(max_lastfm_features, 30, 10, max_lastfm_target+1).to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lastfm_1.parameters())\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(20), desc=\"Training\"):\n",
    "    model_lastfm_1.train()\n",
    "    optimizer.zero_grad()\n",
    "    out:torch.FloatTensor = model_lastfm_1(lastfm_pyg).squeeze(-1)\n",
    "    loss = criterion(\n",
    "        out[lastfm_pyg.train_mask], lastfm_pyg.y[lastfm_pyg.train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.log_softmax(dim=1).argmax(dim=1)\n",
    "        acc = (\n",
    "            (pred[lastfm_pyg.test_mask] == lastfm_pyg.y[lastfm_pyg.test_mask])\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.log_softmax(dim=1).argmax(dim=1)\n",
    "        acc = (\n",
    "            (pred[lastfm_pyg.test_mask] == lastfm_pyg.y[lastfm_pyg.test_mask])\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 1/20 [00:01<00:25,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 3.0046, Test Acc: 0.0354\n",
      "Epoch 000, Loss: 3.0046, Test Acc: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 5/20 [00:05<00:16,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004, Loss: 2.9835, Test Acc: 0.0354\n",
      "Epoch 004, Loss: 2.9835, Test Acc: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 9/20 [00:09<00:11,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008, Loss: 2.9626, Test Acc: 0.0354\n",
      "Epoch 008, Loss: 2.9626, Test Acc: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 13/20 [00:14<00:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012, Loss: 2.9417, Test Acc: 0.0354\n",
      "Epoch 012, Loss: 2.9417, Test Acc: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 17/20 [00:18<00:03,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016, Loss: 2.9206, Test Acc: 0.0354\n",
      "Epoch 016, Loss: 2.9206, Test Acc: 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:22<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "model_lastfm_1 = SAGE(max_lastfm_features, 30, 10, max_lastfm_target+1).to(\"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lastfm_1.parameters())\n",
    "\n",
    "for epoch in tqdm(range(20), desc=\"Training\"):\n",
    "    model_lastfm_1.train()\n",
    "    optimizer.zero_grad()\n",
    "    out:torch.FloatTensor = model_lastfm_1(lastfm_pyg).squeeze(-1)\n",
    "    loss = criterion(\n",
    "        out[lastfm_pyg.train_mask], lastfm_pyg.y[lastfm_pyg.train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.log_softmax(dim=1).argmax(dim=1)\n",
    "        acc = (\n",
    "            (pred[lastfm_pyg.test_mask] == lastfm_pyg.y[lastfm_pyg.test_mask])\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        pred = out.log_softmax(dim=1).argmax(dim=1)\n",
    "        acc = (\n",
    "            (pred[lastfm_pyg.test_mask] == lastfm_pyg.y[lastfm_pyg.test_mask])\n",
    "            .float()\n",
    "            .mean()\n",
    "        )\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss.item():.4f}, Test Acc: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats du GCN sont ici bien meilleurs que ceux du SAGE, ce qui peut s'expliquer par la très grande connectivité de notre graphe. On obtient néanmoins au mieux une accuracy de 11.8% ce qui est bien meilleur que le hasard (1/18 = 5.5%) mais sans pour autant être des résultats impressionnants. On peut néanmoins essayer d'expliquer ce résultats meilleur que le résultat précédent car les goûts musicaux sont plus liés à une zone géographique (ici la tâche étant la classification des nationalités des utilisateurs) qu'au genre. On peut notamment penser à la barrière de la langue, les japonais écoutant davantages de musiques chantées dans leur langue maternelle que d'autres habitants d'asie du sud-est. L'information semble donc ici plus pertinente. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
