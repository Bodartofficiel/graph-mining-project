{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define:    \n",
    "- **The symmetrically normalized Laplacian**:\n",
    "> $L_{sym} = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}$     \n",
    "- **The random walk Laplacian**:\n",
    ">$L_{rw} = D^{-1}L$     \n",
    "\n",
    "## Properties of the random walk Laplacian:\n",
    "> - Smallest eigenvalue is 0:    \n",
    ">   Let us compute $L_{rw}\\mathbf{1} = D^{-1}L\\mathbf{1} = D^{-1}\\mathbf{0} = \\mathbf{0}$    \n",
    ">   where $\\mathbf{1}$ is a column matrix with all entries equal to one.\n",
    "\n",
    "> - Vll eigenvalues are smaller than 2:     \n",
    ">   First, let us show that any eigenvalue of $D^{-1}V$ are between $-1$ and $1$:       \n",
    ">   For any eigenvector $X\\in\\mathcal{R}^n$ of $D^{-1}V$ of eigenvalue $\\lambda$, we can compute $|(D^{-1}VX)_{i_0}|$, where $i_0 = argmax_{i\\in[|0,n|]}|X_i| $       \n",
    ">   $|\\lambda X_{i_0}| = |(D^{-1}VX)_{i_0}| = | d_{ii}^{-1}\\sum_j a_{ij}X_j| ≤ d_{ii}\\sum_j a_{ij}|X_j| ≤ d_{ii}\\sum_j a{ij} |X_{i_0}| = |X_{i_0}|$\n",
    ">   So, $|\\lambda| ≤ 1$     \n",
    ">   We then have $L_{rw}X = X - D^{-1}VX = (1-\\lambda)X $ so any eigenvalue of $L_{rw}$ is between $0$ and $2$.     \n",
    "\n",
    "## Properties of the symmetrically normalized Laplacian:\n",
    "> - Symmetric:    \n",
    ">   $L_{sym}^T = (D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}})^T = (D^{-\\frac{1}{2}})^T L^T (D^{-\\frac{1}{2}})^T = D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}} = L_{sym}$    \n",
    ">   because $D^{-\\frac{1}{2}}$ and $L$ are symmetrical (iff the adjacency matrix is symmetrical *i.e* the graph is not oriented)     \n",
    "\n",
    "> - Positive semi-definite:    \n",
    ">   Let us consider $x\\in\\mathcal{R}^n$     \n",
    ">   $x^TL_{sym}x = (x^TD^{-\\frac{1}{2}})L(D^{-\\frac{1}{2}}x) = y^TLy$, with $y = D^{-\\frac{1}{2}}x\\in\\mathcal{R}^n$     \n",
    ">   Since $L$ is positive semi-definite, $L_{sym}$ is also positive semi-definite     \n",
    "\n",
    "> - The eigenvalues of $L_{rw}$ and $L_{sym}$ are the same:     \n",
    ">   Let us $X$ be an eigenvector of $L_{rw}$ of eigenvalue $\\lambda$. We have:      \n",
    ">   $\\lambda D^{\\frac{1}{2}}X =  D^{\\frac{1}{2}}L_{rw}X = D^{\\frac{1}{2}}D^{-1}LX = (D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}})D^{\\frac{1}{2}}X = L_{sym}D^{\\frac{1}{2}}X $     \n",
    ">   So $D^{\\frac{1}{2}}X$ is an eigenvector of $L_{sym}$, of eigenvalue $\\lambda$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise was solved in the 'Project.ipynb' file in section 4.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to give the formalisation for approximating the RatioCut for an arbitrary $ k $.\n",
    "\n",
    "Let us first redefine the RatioCut:\n",
    "\n",
    "$$\n",
    "\\text{RatioCut}(V_1, V_2, \\ldots, V_k) = \\frac{1}{2} \\sum_{i=1}^{k} \\frac{\\text{cut}(V_i, \\bar{V_i})}{|V_i|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{cut}(V_i, \\bar{V_i})$ is the number of edges crossing the boundary between cluster $ V_i $ and its complementary $\\bar{V_i}$.\n",
    "\n",
    "Then, we define the matrix $ X $, representing the partition, by its vectors $ X^j $ as:\n",
    "\n",
    "$$\n",
    "X^j_i =\n",
    "\\begin{cases}\n",
    "a_j & \\text{if } v_i \\in V_j \\\\\n",
    "0 & \\text{if } v_i \\notin V_j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Combining this vector with Laplacian graph properties, we have for some $ a_j \\neq 0 $:\n",
    "\n",
    "$$\n",
    "(X^j)^T L X^j = a_j^2 (\\text{cut}(V_j, \\overline{V_j}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "(X^j)^T X^j = a_j^2 |V_j|\n",
    "$$\n",
    "\n",
    "Combining these properties, we can rewrite the RatioCut formula as:\n",
    "\n",
    "$$\n",
    "\\text{Rcut}(V_1, \\ldots, V_K) = \\sum_{i=1}^{K} \\frac{(X^j)^T L X^j}{(X^j)^T X^j}\n",
    "$$\n",
    "\n",
    "Let the set $\\mathcal{X}$ be defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{X} = \\left\\{ [X^1 \\ldots X^K] \\mid X^j = a_j (x_1^j, \\ldots, x_N^j), x_i^j \\in \\{1, 0\\}, \\, a_j \\in \\mathbb{R}, \\, X^j \\neq 0 \\right\\}\n",
    "$$\n",
    "\n",
    "This allows us to define the problem of finding $ k $ clusters to minimize the RatioCut as:\n",
    "\n",
    "$$\n",
    "\\text{minimize:} \\quad \\sum_{j=1}^{K} \\frac{(X^j)^T L X^j}{(X^j)^T X^j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:} \\quad (X^j)^T X^j = 0, \\quad 1 \\leq i,j \\leq K, \\quad i \\neq j, \\quad X \\in \\mathcal{X}\n",
    "$$\n",
    "\n",
    "As in the 2-clusters case, we can relax the discreteness condition to get:\n",
    "\n",
    "$$\n",
    "\\text{minimize:} \\quad \\sum_{j=1}^{K} \\frac{(X^j)^T L X^j}{(X^j)^T X^j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{subject to:} \\quad (X^j)^T X^j = 0, \\quad 1 \\leq i,j \\leq K, \\quad i \\neq j, \\quad X \\in \\mathbb{R}^{|V|}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
